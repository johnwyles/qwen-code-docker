# ğŸš€ Qwen-Code Docker

[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Node.js](https://img.shields.io/badge/node.js-6DA55F?style=for-the-badge&logo=node.js&logoColor=white)](https://nodejs.org/)
[![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white)](https://openai.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

> ğŸ³ A containerized development environment for the Qwen-Code CLI with OpenAI-compatible API support

## âœ¨ Features

- ğŸ”§ **Pre-configured Environment** - Ready-to-use Docker container with latest Qwen-Code CLI
- ğŸŒ **OpenAI Compatible** - Works with any OpenAI-compatible API (Ollama, OpenAI, etc.)
- ğŸŒ‰ **Gemini-OpenAI Bridge** - Transparent translation layer for Ollama compatibility
- ğŸ”„ **Intelligent Container Management** - Automatic rebuild detection and state handling
- ğŸ“ **Persistent Storage** - Your work and configuration persist across container restarts
- âš¡ **Latest Versions** - Always uses the latest Node.js, npm, and Qwen-Code CLI
- ğŸ›¡ï¸ **Security First** - Non-root user execution with proper permissions

## ğŸ—ï¸ Architecture

### With Gemini-OpenAI Bridge (Recommended for Ollama)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your Host     â”‚    â”‚ Docker Container â”‚    â”‚  AI Service   â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚               â”‚
â”‚  ./start.sh â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤ qwen-code CLI    â”‚    â”‚               â”‚
â”‚  ./workspace/   â”‚    â”‚ â†“ (Gemini fmt)   â”‚    â”‚               â”‚
â”‚  ./config/      â”‚    â”‚ ğŸŒ‰ Bridge :8080  â”œâ”€â”€â”€â”€â”¤ Ollama Server â”‚
â”‚                 â”‚    â”‚ â†“ (OpenAI fmt)   â”‚    â”‚ (port 11434)  â”‚
â”‚                 â”‚    â”‚ Node.js 22 LTS   â”‚    â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Direct Connection (OpenAI/Compatible APIs)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your Host     â”‚    â”‚ Docker Container â”‚    â”‚  AI Service  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚              â”‚
â”‚  ./start.sh â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤  qwen-code CLI   â”œâ”€â”€â”€â”€â”¤  OpenAI API  â”‚
â”‚  ./workspace/   â”‚    â”‚  Node.js 22 LTS  â”‚    â”‚   (direct)   â”‚
â”‚  ./config/      â”‚    â”‚  Latest npm      â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- ğŸ³ [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/)
- ğŸ¤– An OpenAI-compatible API endpoint (Ollama, OpenAI, etc.)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/your-username/qwen-code-docker.git
   cd qwen-code-docker
   ```

2. **Configure your environment:**
   ```bash
   cp .env.example .env
   nano .env  # Edit with your API settings
   ```

3. **Start the environment:**
   ```bash
   chmod +x start.sh
   ./start.sh
   ```

4. **Choose "OpenAI" when prompted** and start coding! ğŸ‰

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file with your API configuration:

```bash
# API endpoint for your AI service
OPENAI_BASE_URL=http://your-ollama-server:11434/v1

# Your API key (leave empty for local Ollama)
OPENAI_API_KEY=your-api-key-here

# Model name to use
OPENAI_MODEL=qwen3-coder:latest

# For Ollama servers (recommended):
USE_GEMINI_BRIDGE=true
BRIDGE_TARGET_URL=http://your-ollama-server:11434/v1
BRIDGE_PORT=8080
GEMINI_DEFAULT_AUTH_TYPE=openai
```

### Supported Providers

| Provider | Base URL | API Key Required | Bridge Required |
|----------|----------|------------------|-----------------|
| ğŸ¦™ **Local Ollama** | `http://localhost:11434/v1` | No | âœ… Recommended |
| ğŸŒ **Remote Ollama** | `http://your-server:11434/v1` | Optional | âœ… Recommended |
| ğŸ§  **OpenAI** | `https://api.openai.com/v1` | Yes | âŒ Direct |
| ğŸ”— **Custom API** | `https://your-api.com/v1` | Varies | âš ï¸ Depends |

## ğŸ¯ Usage

### Starting the Environment

The intelligent `start.sh` script handles everything automatically:

```bash
./start.sh
```

**What it does:**
- ğŸ” Detects if container exists and its state
- ğŸ”„ Rebuilds image if Dockerfile has changed  
- ğŸ’¾ Preserves your workspace and configuration
- ğŸš€ Launches you directly into the Qwen-Code CLI

### Working with Files

Your project files are mounted and persistent:

```bash
# Host directory          â†’ Container path
./workspace/             â†’ /workspace/code/
./config/                â†’ /workspace/.config/qwen-code/
```

### Common Commands

```bash
# Stop the environment
docker compose down

# View container logs
docker compose logs -f

# Manual container access
docker exec -it qwen-code bash

# Rebuild from scratch
docker compose build --no-cache
```

## ğŸŒ‰ Gemini-OpenAI Bridge

The qwen-code CLI is based on Google's Gemini CLI and sends requests in a hybrid format that includes Gemini-specific fields. This can cause compatibility issues with pure OpenAI-compatible APIs like Ollama.

Our **Gemini-OpenAI Bridge** solves this by:

- ğŸ”„ **Transparent Translation** - Converts Gemini format to clean OpenAI format
- ğŸ›¡ï¸ **Token Limit Protection** - Caps excessive token requests (200k+ â†’ 4k)
- ğŸ§¹ **Field Cleaning** - Removes incompatible Gemini-specific fields
- âš¡ **Zero Configuration** - Automatically starts when `USE_GEMINI_BRIDGE=true`

### When to Use the Bridge

**âœ… Use Bridge:**
- ğŸ¦™ Ollama servers (local or remote)
- ğŸ”§ APIs that expect pure OpenAI format
- âš ï¸ Getting 400 errors with direct connection

**âŒ Direct Connection:**
- ğŸ§  OpenAI official API
- ğŸ”— APIs that handle Gemini format

### Bridge Configuration

Add these to your `.env` file:

```bash
# Enable bridge
USE_GEMINI_BRIDGE=true

# Where the bridge forwards requests
BRIDGE_TARGET_URL=http://your-ollama-server:11434/v1

# Bridge listening port (default: 8080)
BRIDGE_PORT=8080

# Enable debug logging (optional)
BRIDGE_DEBUG=true
```

### Troubleshooting Bridge

```bash
# Check bridge health
curl http://localhost:8080/health

# View bridge logs
docker logs qwen-code | grep Bridge

# Test request translation
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen3-coder:latest","messages":[{"role":"user","content":"test"}]}'
```

For detailed bridge documentation, see [`gemini-openai-bridge/README.md`](gemini-openai-bridge/README.md).

## ğŸ› ï¸ Development

### Project Structure

```
qwen-code-docker/
â”œâ”€â”€ ğŸ“ workspace/           # Your code files (mounted)
â”œâ”€â”€ ğŸ“ config/              # Qwen-Code configuration (mounted)
â”œâ”€â”€ ğŸ“ docs/                # Documentation
â”œâ”€â”€ ğŸ“ gemini-openai-bridge/ # Bridge source code & tests
â”œâ”€â”€ ğŸ“ tests/               # Integration tests
â”œâ”€â”€ ğŸ³ Dockerfile           # Container definition
â”œâ”€â”€ ğŸ³ docker-compose.yml   # Service configuration
â”œâ”€â”€ âš™ï¸ .env                 # Your API settings
â”œâ”€â”€ âš™ï¸ .env.example         # Example configuration
â”œâ”€â”€ ğŸš€ start.sh             # Intelligent startup script
â”œâ”€â”€ ğŸ§ª docker-entrypoint.sh # Container startup script
â””â”€â”€ ğŸ“– README.md            # This file
```

### Container Specifications

- **Base Image:** Ubuntu 22.04 LTS
- **Node.js:** v22 LTS (latest)
- **npm:** Latest version
- **User:** Non-root (`qwen`) for security
- **Ports:** No exposed ports (connects to external APIs)

## ğŸ”§ Troubleshooting

### Common Issues

**ğŸ”´ Connection Error**
```bash
# Check your API endpoint is accessible
curl -v http://your-api-server/v1/models

# Verify environment variables
docker exec qwen-code env | grep OPENAI
```

**ğŸ”´ Permission Denied (Docker)**
```bash
# Add user to docker group
sudo usermod -aG docker $USER
# Log out and back in, or run:
newgrp docker
```

**ğŸ”´ Container Won't Start**
```bash
# Check Docker is running
docker info

# Rebuild from scratch
docker compose down
docker compose build --no-cache
```

### Getting Help

1. ğŸ“– Check the [Configuration Guide](docs/CONFIGURATION.md)
2. ğŸ› Review [Troubleshooting Guide](docs/TROUBLESHOOTING.md)  
3. ğŸ’¬ Open an [issue](https://github.com/your-username/qwen-code-docker/issues)

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch
3. ğŸ“ Make your changes
4. âœ… Test thoroughly
5. ğŸ“¤ Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Qwen-Code CLI](https://github.com/QwenLM/Qwen3-Coder) - The amazing AI coding assistant
- [Ollama](https://ollama.ai/) - Local AI model serving
- [Docker](https://www.docker.com/) - Containerization platform

---

<div align="center">

**â­ Star this repository if it helped you!**

Made with â¤ï¸ for the AI coding community

</div>