# ================================================================
# QWEN-CODE DOCKER ENVIRONMENT CONFIGURATION
# ================================================================
# Copy this file to '.env' and customize for your setup
# DO NOT commit the .env file to version control!

# ================================================================
# üåê API ENDPOINT CONFIGURATION
# ================================================================
# The base URL for your OpenAI-compatible API service
# This should end with '/v1' for OpenAI compatibility

# For local Ollama installation:
OPENAI_BASE_URL=http://localhost:11434/v1

# For remote Ollama server:
# OPENAI_BASE_URL=http://your-server.example.com:11434/v1

# For OpenAI API:
# OPENAI_BASE_URL=https://api.openai.com/v1

# For other providers (Azure OpenAI, etc.):
# OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-model/v1

# ================================================================
# üîë AUTHENTICATION
# ================================================================
# Your API key for authentication
# - Leave empty for local Ollama (no authentication required)
# - Required for OpenAI and most remote services

# For local Ollama (no key needed):
OPENAI_API_KEY=

# For OpenAI:
# OPENAI_API_KEY=sk-your-openai-api-key-here

# For other services:
# OPENAI_API_KEY=your-api-key-here

# ================================================================
# ü§ñ MODEL CONFIGURATION
# ================================================================
# The name of the model to use for code generation
# This must match exactly what your API provider expects

# For Qwen3-Coder models:
OPENAI_MODEL=qwen3-coder:latest

# Force qwen to use OpenAI mode instead of Gemini
GEMINI_DEFAULT_AUTH_TYPE=openai

# Other popular model examples:
# OPENAI_MODEL=qwen3-coder:14b
# OPENAI_MODEL=qwen3-coder:7b
# OPENAI_MODEL=llama3:8b
# OPENAI_MODEL=deepseek-r1:8b
# OPENAI_MODEL=gpt-4
# OPENAI_MODEL=gpt-3.5-turbo

# ================================================================
# üìã COMMON CONFIGURATIONS
# ================================================================

# üè† LOCAL OLLAMA SETUP (RECOMMENDED WITH BRIDGE)
# USE_GEMINI_BRIDGE=true
# OPENAI_BASE_URL=http://localhost:11434/v1
# BRIDGE_TARGET_URL=http://localhost:11434/v1
# BRIDGE_PORT=8080
# OPENAI_API_KEY=
# OPENAI_MODEL=qwen3-coder:latest
# GEMINI_DEFAULT_AUTH_TYPE=openai

# üåê REMOTE OLLAMA SETUP (RECOMMENDED WITH BRIDGE)  
# USE_GEMINI_BRIDGE=true
# OPENAI_BASE_URL=http://your-server.example.com:11434/v1
# BRIDGE_TARGET_URL=http://your-server.example.com:11434/v1
# BRIDGE_PORT=8080
# OPENAI_API_KEY=optional-api-key
# OPENAI_MODEL=qwen3-coder:latest
# GEMINI_DEFAULT_AUTH_TYPE=openai

# üß† OPENAI SETUP
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL=gpt-4

# üîó CUSTOM API SETUP
# OPENAI_BASE_URL=https://your-custom-api.example.com/v1
# OPENAI_API_KEY=your-custom-api-key
# OPENAI_MODEL=your-model-name

# ================================================================
# üîó GEMINI-OPENAI BRIDGE CONFIGURATION
# ================================================================
# Enable the bridge for qwen-code compatibility with Ollama
# Set to 'true' if qwen-code can't connect directly to your server

# Disable bridge (default - for direct OpenAI-compatible servers):
USE_GEMINI_BRIDGE=false

# Enable bridge (for Ollama or other servers that need translation):
# USE_GEMINI_BRIDGE=true
# BRIDGE_TARGET_URL=http://your-ollama-server:11434/v1
# BRIDGE_PORT=8080
# BRIDGE_DEBUG=false

# ================================================================
# üìã BRIDGE USAGE EXAMPLES
# ================================================================

# üè† LOCAL OLLAMA WITH BRIDGE (ALTERNATIVE FORMAT):
# USE_GEMINI_BRIDGE=true
# OPENAI_BASE_URL=http://localhost:11434/v1  # Bridge forwards here
# BRIDGE_TARGET_URL=http://localhost:11434/v1
# BRIDGE_PORT=8080
# OPENAI_API_KEY=
# OPENAI_MODEL=qwen3-coder:latest
# GEMINI_DEFAULT_AUTH_TYPE=openai

# üåê REMOTE OLLAMA WITH BRIDGE (ALTERNATIVE FORMAT):
# USE_GEMINI_BRIDGE=true
# OPENAI_BASE_URL=http://your-server.example.com:8443/v1
# BRIDGE_TARGET_URL=http://your-server.example.com:8443/v1
# BRIDGE_PORT=8080
# OPENAI_API_KEY=your-api-key
# OPENAI_MODEL=qwen3-coder:latest
# GEMINI_DEFAULT_AUTH_TYPE=openai

# üß† OPENAI WITHOUT BRIDGE:
# USE_GEMINI_BRIDGE=false
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL=gpt-4

# ================================================================
# üõ†Ô∏è TROUBLESHOOTING TIPS
# ================================================================
# 
# 1. Make sure your API endpoint is accessible:
#    curl -v http://your-server:11434/v1/models
#
# 2. Verify your model name exists:
#    curl http://your-server:11434/v1/models | grep "your-model"
#
# 3. Test authentication:
#    curl -H "Authorization: Bearer your-key" http://your-server/v1/models
#
# 4. Check container environment:
#    docker exec qwen-code env | grep OPENAI
#
# 5. Test bridge health (if enabled):
#    curl http://localhost:8080/health
#
# 6. Debug bridge requests (if enabled):
#    docker logs qwen-code | grep Bridge
#
# 7. Test bridge translation:
#    curl -X POST http://localhost:8080/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{"model":"qwen3-coder:latest","messages":[{"role":"user","content":"test"}]}'
#
# ================================================================