# Qwen Coder Docker Environment Configuration
# Copy this file to .env and modify the values as needed
# DO NOT commit the .env file to version control

# =============================================================================
# OLLAMA SERVER CONFIGURATION
# =============================================================================

# Ollama server hostname and port
# Default: UNM Alliance endpoint
# Examples:
#   - avi.alliance.unm.edu:8443 (UNM Alliance - default)
#   - localhost:11434 (local Ollama installation)
#   - your-server.com:8443 (custom endpoint)
OLLAMA_HOST=avi.alliance.unm.edu:8443

# Protocol to use for Ollama connection
# Options: http, https
# Use https for remote endpoints, http for local development
OLLAMA_PROTOCOL=https

# API key for Ollama authentication
# REQUIRED for remote endpoints, optional for local installations
# Keep this secret and never commit to version control
OLLAMA_API_KEY=your_api_key_here

# Connection timeout in milliseconds
# Default: 30000 (30 seconds)
OLLAMA_TIMEOUT=30000

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Primary model name to use
# Default: qwen2.5-coder
# Other options: qwen2.5-coder:7b, qwen2.5-coder:14b, qwen2.5-coder:32b
MODEL_NAME=qwen2.5-coder

# Fallback model if primary is unavailable
# Leave empty to disable fallback
FALLBACK_MODEL=

# Model parameters
# Temperature for response randomness (0.0 = deterministic, 1.0 = creative)
MODEL_TEMPERATURE=0.1

# Maximum tokens in response
MODEL_MAX_TOKENS=4096

# Top-p sampling parameter
MODEL_TOP_P=0.9

# =============================================================================
# DOCKER AND APPLICATION CONFIGURATION
# =============================================================================

# Port to expose the application on
# Change this if port 8080 is already in use
CONTAINER_PORT=8080

# Local workspace directory
# This directory will be mounted into the container
WORKSPACE_PATH=./workspace

# Container name
# Useful for multiple instances
CONTAINER_NAME=qwen-code

# Docker network name
# Leave empty to use default network
DOCKER_NETWORK=

# =============================================================================
# DEVELOPMENT AND DEBUGGING
# =============================================================================

# Log level for application
# Options: debug, info, warn, error
LOG_LEVEL=info

# Enable debug mode
# Set to true for verbose logging and development features
DEBUG_MODE=false

# Enable API request logging
# Logs all requests to Ollama (may include sensitive data)
LOG_API_REQUESTS=false

# Enable performance monitoring
# Tracks response times and resource usage
ENABLE_MONITORING=false

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# Enable CORS for web interface
# Set to false in production if not needed
ENABLE_CORS=true

# Allowed CORS origins (comma-separated)
# Use * for development, specific domains for production
CORS_ORIGINS=*

# Enable request rate limiting
# Helps prevent abuse of the API
ENABLE_RATE_LIMITING=true

# Rate limit: requests per minute
RATE_LIMIT_PER_MINUTE=60

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================

# Custom headers for Ollama requests (JSON format)
# Example: {"X-Custom-Header": "value"}
CUSTOM_HEADERS={}

# HTTP proxy configuration (if needed)
# Example: http://proxy.company.com:8080
HTTP_PROXY=

# HTTPS proxy configuration (if needed)
HTTPS_PROXY=

# No proxy hosts (comma-separated)
# Hosts that should bypass proxy
NO_PROXY=localhost,127.0.0.1

# =============================================================================
# BACKUP AND PERSISTENCE
# =============================================================================

# Enable automatic workspace backup
ENABLE_BACKUP=false

# Backup interval in hours
BACKUP_INTERVAL=24

# Backup retention days
BACKUP_RETENTION_DAYS=7

# Backup location
BACKUP_PATH=./backups

# =============================================================================
# INTEGRATION SETTINGS
# =============================================================================

# Git configuration for workspace
GIT_USER_NAME=
GIT_USER_EMAIL=

# Code editor preferences
PREFERRED_EDITOR=vscode

# Language server configurations
ENABLE_LANGUAGE_SERVERS=true

# =============================================================================
# EXAMPLES FOR DIFFERENT SETUPS
# =============================================================================

# Example 1: Local Ollama installation
# OLLAMA_HOST=localhost:11434
# OLLAMA_PROTOCOL=http
# OLLAMA_API_KEY=
# DEBUG_MODE=true

# Example 2: University/Institution endpoint
# OLLAMA_HOST=ml-server.university.edu:8443
# OLLAMA_PROTOCOL=https
# OLLAMA_API_KEY=your_institution_api_key
# LOG_LEVEL=info

# Example 3: Cloud-hosted Ollama
# OLLAMA_HOST=your-cloud-instance.com:8443
# OLLAMA_PROTOCOL=https
# OLLAMA_API_KEY=your_cloud_api_key
# ENABLE_RATE_LIMITING=true
# RATE_LIMIT_PER_MINUTE=30

# Example 4: Development with custom model
# MODEL_NAME=qwen2.5-coder:14b
# MODEL_TEMPERATURE=0.2
# DEBUG_MODE=true
# LOG_API_REQUESTS=true

# =============================================================================
# NOTES
# =============================================================================
# 
# 1. Always copy this file to .env before making changes
# 2. Never commit your .env file to version control
# 3. Keep your API keys secure and rotate them regularly
# 4. Test your configuration with: node tests/test-connection.js
# 5. Check Docker logs if issues occur: docker-compose logs
# 6. For local development, you may not need an API key
# 7. Adjust timeout values based on your network conditions
# 8. Enable monitoring and logging for troubleshooting
# 9. Use specific CORS origins in production environments
# 10. Backup your workspace regularly in production use